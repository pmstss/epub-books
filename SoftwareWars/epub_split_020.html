<?xml version='1.0' encoding='utf-8'?>
<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <meta http-equiv="CONTENT-TYPE" content="text/html; charset=utf-8"/>
    <title>After The Software Wars</title>
    <meta name="AUTHOR" content="Keith Curtis"/>
    <meta content="http://www.w3.org/1999/xhtml; charset=utf-8" http-equiv="Content-Type"/><link href="stylesheet.css" type="text/css" rel="stylesheet"/><style type="text/css">
		@page { margin-bottom: 5.000000pt; margin-top: 5.000000pt; }</style></head>
  <body dir="LTR" class="calibre">
<h2 class="calibre2" id="calibre_pb_20"><a name="1.3.Software and the Singularity|outline" id="1.3.Software and the Singularity|outline"></a>
 		Software and the Singularity</h2> 	  <p class="calibre3">Futurists talk about the “Singularity”, the time when computational capacity will surpass the capacity of human intelligence. Ray Kurzweil predicts it will happen in 2045.<a name="sdfootnote17anc" href="epub_split_112.html#sdfootnote17sym" id="sdfootnote17anc"><sup class="calibre7">3</sup></a>
 The flaw with any date estimate, other than the fact that they are always prone to extreme error, is that our software today has no learning capacity, because the idea of continuous learning is not yet a part of the foundation. Even the learning capabilities of an ant would be useful.</p>
 <p class="calibre3">I believe the benefits inherent in the singularity will happen as soon as our software becomes “smart”. I don't believe we need to wait for any further Moore's law progress for that to happen. Computers today can do <span>billions </span>of operations per second, like add 123,456,789 and 987,654,321. Even if you could do that calculation in your head in one second, it would take you 30 years to do <span>the billion </span>that your computer can do in that second.</p>
 <p class="calibre3">Even if you don't think computers have the necessary hardware horsepower to be smart today, understand that in many scenarios,  the size of the input is the driving factor to the processing power required. In image recognition, for example, the amount of work required to interpret an image is mostly a function of the size of the image. Each step in the image recognition pipeline, and the processes that take place in our brain, dramatically reduce the amount of data from the previous step. At the beginning of the analysis might be a one million pixel image, requiring 3 million bytes of memory. At the end of the analysis is the data that you are looking at your house, a concept that requires only 10 bytes to represent. The first step, working on the raw image, requires the most processing power, so therefore it is the image resolution (and frame rate) that set the requirements, values that are trivial to change. No one has shown robust vision recognition software running at any speed, on any sized image!</p>
 <p class="calibre3">While a brain is different from a computer in that it does work in parallel, such parallelization only makes it happen faster, it does not change the result. Anything accomplished in our parallel brain could also be accomplished on computers of today, which can do only one thing at a time, but at the rate of billions per second. A 1-gigahertz processor can do 1,000 different operations on a million pieces of data in one second. With such speed, you don't even need multiple processors! Even so, more parallelism is coming.<a name="sdfootnote18anc" href="epub_split_112.html#sdfootnote18sym" id="sdfootnote18anc"><sup class="calibre7">4</sup></a>
 Once we build software as smart as an ant, we will build software as smart as a human the same day, because it is the same software.</p>
  	 		</body>
</html>
